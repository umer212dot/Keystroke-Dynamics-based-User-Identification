{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN/coTwXotavgMuzt3lUxp+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"XNh0kEDNJpRy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767533600511,"user_tz":-300,"elapsed":9020,"user":{"displayName":"k233012 Umer Ahmed Shaikh","userId":"04131367829639838715"}},"outputId":"5de61406-771c-4c33-bce1-6d22118dcebe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded Train.xlsx and Test.xlsx\n","Train shape: (850, 23) Test shape: (170, 23)\n","Using features: ['avgHoldTime', 'medianIKD', 'holdTimeStdDev', 'tempoChangeRate', 'typingSpeedWPM', 'entropyIKD', 'maxBurstLength', 'commonDigraphTiming', 'skewnessIKD', 'ikdStdDev', 'correctionLatencyMean', 'backspaceRatio']\n","Saved: outputs/kmeans_pca2_scatter.png\n","\n","PCA explained variance ratio: [0.35055392 0.14166348]\n","Saved: outputs/pca_2_user_scatter.png\n","Saved: outputs/pca_scree.png\n","\n","MLP metrics: {'accuracy': 0.9647058823529412, 'precision_macro': 0.9663101604278076, 'recall_macro': 0.9647058823529411, 'f1_macro': 0.9646616541353383}\n","Classification report (MLP):\n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        10\n","           1       0.91      1.00      0.95        10\n","           2       0.91      1.00      0.95        10\n","           3       1.00      1.00      1.00        10\n","           4       1.00      1.00      1.00        10\n","           5       0.90      0.90      0.90        10\n","           6       1.00      0.90      0.95        10\n","           7       1.00      1.00      1.00        10\n","           8       1.00      1.00      1.00        10\n","           9       1.00      0.90      0.95        10\n","          10       0.91      1.00      0.95        10\n","          11       0.80      0.80      0.80        10\n","          12       1.00      1.00      1.00        10\n","          13       1.00      0.90      0.95        10\n","          14       1.00      1.00      1.00        10\n","          15       1.00      1.00      1.00        10\n","          16       1.00      1.00      1.00        10\n","\n","    accuracy                           0.96       170\n","   macro avg       0.97      0.96      0.96       170\n","weighted avg       0.97      0.96      0.96       170\n","\n","\n","Perceptron metrics: {'accuracy': 0.8529411764705882, 'precision_macro': 0.8807539682539682, 'recall_macro': 0.8529411764705882, 'f1_macro': 0.8498036171319726}\n","\n","Linear Regression metrics: {'MSE': 17.55624541413489, 'RMSE': np.float64(4.190017352486131), 'R2': 0.26848977441104627, 'accuracy': 0.12941176470588237}\n","Gradient Descent metrics: {'MSE': 17.555519858168083, 'RMSE': np.float64(4.189930770092518), 'R2': 0.2685200059096632, 'accuracy': 0.12941176470588237}\n","Saved: outputs/gd_loss_curve.png\n","\n","All models comparison:\n","               model  accuracy  precision_macro  recall_macro  f1_macro\n","0     MLPClassifier  0.964706         0.966310      0.964706  0.964662\n","1        Perceptron  0.852941         0.880754      0.852941  0.849804\n","2  LinearRegression  0.129412              NaN           NaN       NaN\n","3   GradientDescent  0.129412              NaN           NaN       NaN\n","\n","Regression models comparison:\n","               model        MSE      RMSE       R2  accuracy\n","0  LinearRegression  17.556245  4.190017  0.26849  0.129412\n","1   GradientDescent  17.555520  4.189931  0.26852  0.129412\n","Saved: outputs/all_models_accuracy_comparison.png\n","Saved: outputs/regression_rmse_comparison.png\n","\n","Saved test sample predictions (MLP).\n","Saved results_summary.txt\n","\n","All finished. Check the outputs/ directory for plots, CSVs, and lecture_notes.md\n"]}],"source":["import os\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans\n","from sklearn.linear_model import LinearRegression, Perceptron\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import (\n","    mean_squared_error, r2_score,\n","    accuracy_score, precision_score, recall_score, f1_score, classification_report\n",")\n","from sklearn.model_selection import train_test_split\n","\n","# Create output directory\n","OUTDIR = \"outputs\"\n","os.makedirs(OUTDIR, exist_ok=True)\n","\n","# Utility functions\n","def savefig(fname):\n","    full = os.path.join(OUTDIR, fname)\n","    plt.savefig(full, dpi=300, bbox_inches='tight')\n","    print(\"Saved:\", full)\n","\n","def rmse(y_true, y_pred):\n","    return np.sqrt(mean_squared_error(y_true, y_pred))\n","\n","# Load data (Train.xlsx expected)\n","try:\n","    df_train = pd.read_excel(\"Train.xlsx\")\n","    df_test = pd.read_excel(\"Test.xlsx\")\n","    print(\"Loaded Train.xlsx and Test.xlsx\")\n","except Exception as e:\n","    print(\"Could not load Excel files; using generated sample data. Error:\", e)\n","    np.random.seed(42)\n","    n = 400\n","    df_train = pd.DataFrame({\n","        'name': np.random.choice([f\"user_{i:02d}\" for i in range(1, 11)], n),\n","        'typingSpeedWPM': np.random.normal(45, 12, n),\n","        'avgHoldTime': np.random.normal(100, 25, n),\n","        'commonDigraphTiming': np.random.normal(80, 30, n),\n","        'correctionLatencyMean': np.random.normal(400, 150, n),\n","        'entropyIKD': np.random.normal(0.9, 0.12, n),\n","        'holdTimeStdDev': np.random.normal(40, 12, n),\n","        'skewnessIKD': np.random.normal(1.6, 0.7, n),\n","        'tempoChangeRate': np.random.normal(0.7, 0.15, n),\n","        'backspaceRatio': np.random.beta(2, 20, n),\n","        'medianIKD': np.random.normal(200, 60, n),\n","        'maxBurstLength': np.random.normal(50, 20, n),\n","        'ikdStdDev': np.random.normal(140, 50, n),\n","    })\n","    # create test set by sampling\n","    df_test = df_train.sample(frac=0.2, random_state=1).reset_index(drop=True)\n","    df_train = df_train.drop(df_test.index).reset_index(drop=True)\n","\n","print(\"Train shape:\", df_train.shape, \"Test shape:\", df_test.shape)\n","\n","# Feature selection\n","features = [\n","    \"avgHoldTime\", \"medianIKD\", \"holdTimeStdDev\", \"tempoChangeRate\",\n","    \"typingSpeedWPM\", \"entropyIKD\", \"maxBurstLength\", \"commonDigraphTiming\",\n","    \"skewnessIKD\", \"ikdStdDev\", \"correctionLatencyMean\", \"backspaceRatio\"\n","]\n","\n","# ensure features exist\n","features = [f for f in features if f in df_train.columns]\n","print(\"Using features:\", features)\n","\n","target_label = 'name'\n","\n","# Encode labels + split\n","encoder = LabelEncoder()\n","encoder.fit(pd.concat([df_train[target_label], df_test[target_label]]))\n","df_train['label_enc'] = encoder.transform(df_train[target_label])\n","df_test['label_enc'] = encoder.transform(df_test[target_label])\n","\n","# Build X, y for predicting user name\n","X = df_train[features].copy()\n","y = df_train['label_enc'].copy()\n","\n","# scale features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# train/test split for model evaluation\n","X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# Batch Gradient Descent (custom) for linear regression\n","def batch_gradient_descent(X, y, lr=0.001, n_iters=5000, add_intercept=True, verbose=False):\n","    Xb = np.copy(X)\n","    if add_intercept:\n","        Xb = np.hstack([np.ones((Xb.shape[0], 1)), Xb])  # add bias column\n","    n_samples, n_features = Xb.shape\n","    theta = np.zeros(n_features)\n","    history = []\n","    for i in range(n_iters):\n","        preds = Xb.dot(theta)\n","        error = preds - y\n","        grad = (1.0 / n_samples) * Xb.T.dot(error)\n","        theta -= lr * grad\n","        if i % 500 == 0:\n","            loss = (1.0 / (2*n_samples)) * np.sum(error ** 2)\n","            history.append((i, loss))\n","            if verbose:\n","                print(f\"Iter {i:5d}, Loss: {loss:.6f}\")\n","    return theta, history\n","\n","# K-Means Clustering (unsupervised)\n","k = 17  # number of clusters up to number of users\n","kmeans = KMeans(n_clusters=k, random_state=42)\n","kmeans.fit(X_scaled)\n","cluster_labels = kmeans.predict(X_scaled)\n","\n","# Add clusters to a PCA 2D plot later\n","numOfPC = 2\n","pca2 = PCA(n_components=numOfPC)\n","X_pca2 = pca2.fit_transform(X_scaled)\n","\n","plt.figure(figsize=(8,6))\n","sns.scatterplot(x=X_pca2[:,0], y=X_pca2[:,1], hue=cluster_labels, palette='tab10', legend='full', s=40)\n","plt.title(f'KMeans clusters (k={k}) on PCA({numOfPC})')\n","savefig(\"kmeans_pca2_scatter.png\")\n","plt.close()\n","\n","# PCA (for visualization & dimensionality reduction)\n","pca = PCA(n_components=numOfPC)\n","pc = pca.fit_transform(X_scaled)\n","explained = pca.explained_variance_ratio_\n","print(\"\\nPCA explained variance ratio:\", explained)\n","\n","plt.figure(figsize=(8,6))\n","sns.scatterplot(x=pc[:,0], y=pc[:,1], hue=df_train['label_enc'], palette='tab10', s=40)\n","plt.title(f'PCA ({numOfPC} components) of features colored by user label')\n","plt.xlabel(f'PC1 ({explained[0]*100:.1f}%)')\n","plt.ylabel(f'PC2 ({explained[1]*100:.1f}%)')\n","savefig(\"pca_2_user_scatter.png\")\n","plt.close()\n","\n","# Scree plot\n","plt.figure(figsize=(6,4))\n","components = np.arange(1, len(pca.explained_variance_ratio_)+1)\n","plt.bar(components, pca.explained_variance_ratio_)\n","plt.xlabel(\"Principal component\")\n","plt.ylabel(\"Explained variance ratio\")\n","plt.title(\"PCA Scree Plot\")\n","savefig(\"pca_scree.png\")\n","plt.close()\n","\n","# Neural Network (MLPClassifier) for user classification\n","\n","# We'll use a small MLP with one hidden layer\n","mlp = MLPClassifier(hidden_layer_sizes=(64,), activation='relu', max_iter=1000, random_state=42)\n","mlp.fit(X_train, y_train)\n","y_pred_mlp = mlp.predict(X_val)\n","\n","mlp_metrics = {\n","    'accuracy': accuracy_score(y_val, y_pred_mlp),\n","    'precision_macro': precision_score(y_val, y_pred_mlp, average='macro', zero_division=0),\n","    'recall_macro': recall_score(y_val, y_pred_mlp, average='macro', zero_division=0),\n","    'f1_macro': f1_score(y_val, y_pred_mlp, average='macro', zero_division=0)\n","}\n","print(\"\\nMLP metrics:\", mlp_metrics)\n","print(\"Classification report (MLP):\\n\", classification_report(y_val, y_pred_mlp, zero_division=0))\n","\n","# Perceptron (simple linear classifier)\n","per = Perceptron(max_iter=1000, random_state=42)\n","per.fit(X_train, y_train)\n","y_pred_per = per.predict(X_val)\n","per_metrics = {\n","    'accuracy': accuracy_score(y_val, y_pred_per),\n","    'precision_macro': precision_score(y_val, y_pred_per, average='macro', zero_division=0),\n","    'recall_macro': recall_score(y_val, y_pred_per, average='macro', zero_division=0),\n","    'f1_macro': f1_score(y_val, y_pred_per, average='macro', zero_division=0)\n","}\n","print(\"\\nPerceptron metrics:\", per_metrics)\n","\n","# Linear Regression for user prediction (treating encoded label as continuous)\n","linreg = LinearRegression()\n","linreg.fit(X_train, y_train.values)\n","y_pred_lin = linreg.predict(X_val)\n","# Round to nearest integer and clip to valid label range\n","y_pred_lin_rounded = np.clip(np.round(y_pred_lin).astype(int), 0, len(encoder.classes_)-1)\n","\n","lin_metrics = {\n","    'MSE': mean_squared_error(y_val, y_pred_lin),\n","    'RMSE': rmse(y_val, y_pred_lin),\n","    'R2': r2_score(y_val, y_pred_lin),\n","    'accuracy': accuracy_score(y_val, y_pred_lin_rounded)\n","}\n","print(\"\\nLinear Regression metrics:\", lin_metrics)\n","\n","# Gradient Descent for user prediction\n","theta, history = batch_gradient_descent(X_train, y_train.values, lr=0.01, n_iters=5000, verbose=False)\n","X_val_b = np.hstack([np.ones((X_val.shape[0],1)), X_val])\n","y_pred_gd = X_val_b.dot(theta)\n","# Round to nearest integer and clip to valid label range\n","y_pred_gd_rounded = np.clip(np.round(y_pred_gd).astype(int), 0, len(encoder.classes_)-1)\n","\n","gd_metrics = {\n","    'MSE': mean_squared_error(y_val, y_pred_gd),\n","    'RMSE': rmse(y_val, y_pred_gd),\n","    'R2': r2_score(y_val, y_pred_gd),\n","    'accuracy': accuracy_score(y_val, y_pred_gd_rounded)\n","}\n","print(\"Gradient Descent metrics:\", gd_metrics)\n","\n","# save GD loss curve\n","iters, losses = zip(*history)\n","plt.figure(figsize=(6,4))\n","plt.plot(iters, losses)\n","plt.xlabel(\"Iteration\")\n","plt.ylabel(\"Loss (1/(2n) sum sq)\")\n","plt.title(\"GD Loss Curve\")\n","savefig(\"gd_loss_curve.png\")\n","plt.close()\n","\n","# Summary comparisons and plots\n","\n","# All models comparison (accuracy)\n","all_models_table = pd.DataFrame({\n","    'model': ['MLPClassifier', 'Perceptron', 'LinearRegression', 'GradientDescent'],\n","    'accuracy': [mlp_metrics['accuracy'], per_metrics['accuracy'],\n","                 lin_metrics['accuracy'], gd_metrics['accuracy']],\n","    'precision_macro': [mlp_metrics['precision_macro'], per_metrics['precision_macro'],\n","                       np.nan, np.nan],  # Regression models don't have precision/recall/f1\n","    'recall_macro': [mlp_metrics['recall_macro'], per_metrics['recall_macro'],\n","                     np.nan, np.nan],\n","    'f1_macro': [mlp_metrics['f1_macro'], per_metrics['f1_macro'],\n","                 np.nan, np.nan]\n","})\n","all_models_table.to_csv(os.path.join(OUTDIR, \"all_models_comparison.csv\"), index=False)\n","print(\"\\nAll models comparison:\\n\", all_models_table)\n","\n","# Regression models comparison (MSE, RMSE, R2)\n","reg_table = pd.DataFrame({\n","    'model': ['LinearRegression', 'GradientDescent'],\n","    'MSE': [lin_metrics['MSE'], gd_metrics['MSE']],\n","    'RMSE': [lin_metrics['RMSE'], gd_metrics['RMSE']],\n","    'R2': [lin_metrics['R2'], gd_metrics['R2']],\n","    'accuracy': [lin_metrics['accuracy'], gd_metrics['accuracy']]\n","})\n","reg_table.to_csv(os.path.join(OUTDIR, \"regression_comparison.csv\"), index=False)\n","print(\"\\nRegression models comparison:\\n\", reg_table)\n","\n","# Save a barplot comparing all model accuracies\n","plt.figure(figsize=(10,5))\n","sns.barplot(x='model', y='accuracy', data=all_models_table)\n","plt.ylim(0,1)\n","plt.title(\"User prediction accuracy comparison (all models)\")\n","plt.xticks(rotation=45, ha='right')\n","savefig(\"all_models_accuracy_comparison.png\")\n","plt.close()\n","\n","# Save regression RMSE comparison\n","plt.figure(figsize=(8,5))\n","sns.barplot(x='model', y='RMSE', data=reg_table)\n","plt.title(\"Regression models RMSE comparison\")\n","savefig(\"regression_rmse_comparison.png\")\n","plt.close()\n","\n","# Predictions examples (on test set)\n","\n","# Prepare test scaled data\n","X_test_full = scaler.transform(df_test[features])\n","# Predict users with MLP\n","y_test_pred_mlp = mlp.predict(X_test_full)\n","# Map back to names\n","pred_names_mlp = encoder.inverse_transform(y_test_pred_mlp)\n","df_test['pred_mlp'] = pred_names_mlp\n","\n","# Show a few\n","df_test_sample = df_test[[target_label] + features + ['pred_mlp']].head(10)\n","df_test_sample.to_csv(os.path.join(OUTDIR, \"test_predictions_mlp_sample.csv\"), index=False)\n","print(\"\\nSaved test sample predictions (MLP).\")\n","\n","# Save results summary\n","with open(os.path.join(OUTDIR, \"results_summary.txt\"), \"w\", encoding='utf-8') as f:\n","    f.write(\"Linear Regression metrics:\\n\")\n","    f.write(str(lin_metrics) + \"\\n\\n\")\n","    f.write(\"Gradient Descent metrics:\\n\")\n","    f.write(str(gd_metrics) + \"\\n\\n\")\n","    f.write(\"MLP metrics:\\n\")\n","    f.write(str(mlp_metrics) + \"\\n\\n\")\n","    f.write(\"Perceptron metrics:\\n\")\n","    f.write(str(per_metrics) + \"\\n\\n\")\n","print(\"Saved results_summary.txt\")\n","\n","print(\"\\nAll finished. Check the outputs/ directory for plots, CSVs, and lecture_notes.md\")\n"]}]}